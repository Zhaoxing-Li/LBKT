{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99f5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import psutil\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39db5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = 320000\n",
    "\n",
    "MAX_SEQ = 100\n",
    "MIN_SAMPLES = 5\n",
    "EMBED_DIM = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_LEARNING_RATE = 2e-3\n",
    "# EPOCHS = 30\n",
    "EPOCHS = 10\n",
    "# TRAIN_BATCH_SIZE = 2048\n",
    "TRAIN_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca627e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80833b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number skills 13523\n",
      "TRAIN_SAMPLES 314924\n",
      "train_group \n",
      " user_id\n",
      "115     ([5692, 5716, 128, 7860, 7922, 156, 51, 50, 78...\n",
      "124     ([7900, 7876, 175, 1278, 2065, 2063, 2064, 336...\n",
      "2746    ([5273, 758, 5976, 236, 404, 382, 405, 873, 53...\n",
      "5382    ([5000, 3944, 217, 5844, 5965, 4990, 5235, 605...\n",
      "8623    ([3915, 4750, 6456, 3968, 6104, 5738, 6435, 54...\n",
      "dtype: object\n",
      "valid_group \n",
      " user_id\n",
      "1720820513    ([3849, 1320, 5285, 8918, 3644, 6111, 8397, 94...\n",
      "1720823127    ([7900, 7876, 175, 1278, 2064, 2065, 2063, 336...\n",
      "1720823509    ([128, 7860, 7922, 156, 51, 50, 7896, 7863, 15...\n",
      "1720827508    ([7900, 7876, 175, 1278, 2065, 2063, 2064, 336...\n",
      "1720827841    ([7900, 7876, 175, 1278, 2065, 2063, 2064, 336...\n",
      "dtype: object\n",
      "314924 78732\n",
      "CPU times: user 56.7 s, sys: 7.02 s, total: 1min 3s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dtypes = {'timestamp': 'int64', 'user_id': 'int32' ,'content_id': 'int16','content_type_id': 'int8','answered_correctly':'int8'}\n",
    "# train_df = pd.read_feather('../input/riiid-cross-validation-dataset/train.feather')[[\n",
    "#     'timestamp', 'user_id', 'content_id', 'content_type_id', 'answered_correctly'\n",
    "# ]]\n",
    "train_df = pd.read_csv('./input/riiid-test-answer-prediction/train.csv')[['timestamp', 'user_id', 'content_id', 'content_type_id', 'answered_correctly']]\n",
    "for col, dtype in dtypes.items():\n",
    "    train_df[col] = train_df[col].astype(dtype)\n",
    "    \n",
    "    \n",
    "#train_df have only rows with False in content_type_id (0 if the event was a question being posed to the user)\n",
    "train_df = train_df[train_df.content_type_id == False]  \n",
    "\n",
    "train_df = train_df.sort_values(['timestamp'], ascending=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "skills = train_df[\"content_id\"].unique()\n",
    "# joblib.dump(skills, \"skills.pkl.zip\")\n",
    "n_skill = len(skills)  # (unique content IDs)\n",
    "print(\"number skills\", n_skill)\n",
    "\n",
    "\n",
    "group = train_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values))\n",
    "# joblib.dump(group, \"group.pkl.zip\")  # Save models?\n",
    "del train_df\n",
    "gc.collect()\n",
    "# group\n",
    "\n",
    "# The training data is sorted by timestamp and split into two sets using an 80/20 split\n",
    "TRAIN_SAMPLES = int(len(group.index)*0.8)\n",
    "print('TRAIN_SAMPLES',TRAIN_SAMPLES)\n",
    "\n",
    "\n",
    "# The method then creates a dictionary of samples, where each key is a user ID and the corresponding value is a tuple containing the user's content IDs and answered correctly values.\n",
    "train_indexes = list(group.index)[:TRAIN_SAMPLES]\n",
    "valid_indexes = list(group.index)[TRAIN_SAMPLES:]\n",
    "train_group = group[group.index.isin(train_indexes)]\n",
    "valid_group = group[group.index.isin(valid_indexes)]\n",
    "print('train_group \\n', train_group[:5] )\n",
    "print('valid_group \\n', valid_group[:5] )\n",
    "\n",
    "del group, train_indexes, valid_indexes\n",
    "print(len(train_group), len(valid_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6aa38c",
   "metadata": {},
   "source": [
    "### The BERTDataset class returns three values for each sample in the dataset:\n",
    "\n",
    "- `x`: the input data for the BERT model, which consists of the content IDs for each sample shifted by one and the answered correctly values added to the content IDs\n",
    "- `target_id`: the target IDs for each sample, which consist of the content IDs shifted by one\n",
    "- `label`: the labels for each sample, which consist of the answered correctly values shifted by one\n",
    "\n",
    "These values are used as input to the BERT model and are used to calculate the model's performance during training and evaluation. The `x` and `target_id` arrays are used as input to the BERT model, while the `label` array is used to calculate the model's loss and accuracy.\n",
    "\n",
    "The __init__ method iterates over the users in the `group` object and retrieves the questions and answers for each user. If a user has answered fewer than `min_samples` questions, their questions and answers are not included in the `samples` dictionary. If a user has answered more than `max_seq` questions, their questions and answers are split into multiple sequences of length `max_seq` and each sequence is added to the `samples` dictionary using a unique key that includes the user's ID and the sequence number. For example, if the user's ID is `123` and they have answered 150 questions, their questions and answers will be split into two sequences with lengths 128 and 22, and the keys `123_0` and `123_1` will be added to the samples dictionary with the values of the first and second sequence, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5f96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, group, n_skill, min_samples=1, max_seq=128):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.n_skill = n_skill\n",
    "        self.samples = {}\n",
    "        \n",
    "        self.user_ids = []\n",
    "        for user_id in group.index:\n",
    "            q, qa = group[user_id]  # q:content_id(questions); qa:answered_correctly(user's question answer)\n",
    "            if len(q) < min_samples:  # If a user has answered fewer than min_samples questions, their questions and answers are not included in the 'samples' dictionary\n",
    "                continue \n",
    "            \n",
    "            # Main Contribution\n",
    "            if len(q) > self.max_seq:\n",
    "                total_questions = len(q)\n",
    "                initial = total_questions % self.max_seq\n",
    "                if initial >= min_samples:\n",
    "                    self.user_ids.append(f\"{user_id}_0\")\n",
    "                    self.samples[f\"{user_id}_0\"] = (q[:initial], qa[:initial])\n",
    "                for seq in range(total_questions // self.max_seq):\n",
    "                    self.user_ids.append(f\"{user_id}_{seq+1}\")\n",
    "                    start = initial + seq * self.max_seq\n",
    "                    end = start + self.max_seq\n",
    "                    self.samples[f\"{user_id}_{seq+1}\"] = (q[start:end], qa[start:end])\n",
    "            else:\n",
    "                user_id = str(user_id)\n",
    "                self.user_ids.append(user_id)\n",
    "                self.samples[user_id] = (q, qa)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.user_ids[index]\n",
    "        q_, qa_ = self.samples[user_id]\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        if seq_len == self.max_seq:\n",
    "            q[:] = q_\n",
    "            qa[:] = qa_\n",
    "        else:\n",
    "            q[-seq_len:] = q_\n",
    "            qa[-seq_len:] = qa_\n",
    "        \n",
    "        # 'x' also has a length of max_seq-1\n",
    "        target_id = q[1:]  \n",
    "        label = qa[1:]\n",
    "        \n",
    "        x = np.zeros(self.max_seq-1, dtype=int)\n",
    "        x = q[:-1].copy()\n",
    "        x += (qa[:-1] == 1) * self.n_skill  # the model needs to be able to distinguish between the question IDs and the correct answers in order to make predictions.\n",
    "\n",
    "        return x, target_id, label\n",
    "    \n",
    "    \n",
    "train_dataset = BERTDataset(train_group, n_skill, min_samples=MIN_SAMPLES, max_seq=MAX_SEQ)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valid_dataset = BERTDataset(valid_group, n_skill, max_seq=MAX_SEQ)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2eb87",
   "metadata": {},
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0755b2d8",
   "metadata": {},
   "source": [
    "\n",
    "- the `d_model` parameter specifies the size of the hidden states used by the model. This is also known as the \"model size\" or the \"embedding size\" of the model.\n",
    "- The `d_model` parameter is used as a scaling factor when computing the dot product between the query and key vectors in the Attention mechanism. It is also used to specify the size of the input and output vectors for the linear layers in the MultiHeadedAttention class, as well as the size of the input and output vectors for the LayerNorm and SublayerConnection classes. In general, a larger d_model value will result in a more expressive BERT model, but will also increase the computational complexity and memory usage of the model.\n",
    "\n",
    "\n",
    "\n",
    "### Sublayer Connection:\n",
    "- ### residual connection:\n",
    "    - A residual connection is a type of connection in a neural network that allows information to bypass one or more layers of the network. This allows the network to learn to perform tasks more efficiently by allowing the information to flow more directly from the input to the output layers. Residual connections can help improve the performance of the network, particularly on tasks that require the network to process long sequences of data. They are often used in deep learning networks, where they can help prevent the vanishing gradient problem, allowing the network to learn more effectively.\n",
    "    \n",
    "- ### LayerNorm:\n",
    "    - the `eps` parameter specifies a small value used to stabilize the division operation in the layer normalization computation. This is necessary because division by zero is undefined, and division by a very small value can lead to numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca561c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention' mechanism used by BERT\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "# from .single import Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    This class extends the Attention class to support multiple \"heads\" for improved performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "    \n",
    "# implements layer normalization for BERT\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "# implements the residual connections and layer normalization used by BERT to improve training\n",
    "# from .layer_norm import LayerNorm    \n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "\n",
    "# activation function used by BERT\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "# from .gelu import GELU\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "    \n",
    "    \n",
    "\n",
    "# from .attention import MultiHeadedAttention\n",
    "# from .utils import SublayerConnection, PositionwiseFeedForward\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# Copy from https://github.com/codertimo/BERT-pytorch\n",
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, hidden=128, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "        \n",
    "        # paper noted they used 4*hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n",
    "#         self.embedding = BERTEmbedding(vocab_size=2*vocab_size+1, embed_size=hidden)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "        \n",
    "        self.pred = nn.Linear(hidden, 1)\n",
    "        \n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "        \n",
    "        x = self.pred(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# # Copy from chatGPT\n",
    "# class BERTEmbedding(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Define and initialize the embedding layers\n",
    "#         self.word_embedding = nn.Embedding(num_embeddings, d_model)\n",
    "#         self.position_embedding = nn.Embedding(num_positions, d_model)\n",
    "#         self.token_type_embedding = nn.Embedding(num_token_types, d_model)\n",
    "\n",
    "#         # Define the dropout layer\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Perform the model's forward pass\n",
    "#         seq_length, batch_size = x.size()\n",
    "\n",
    "#         # Get the word, position, and token type embeddings\n",
    "#         word_embedding = self.word_embedding(x)\n",
    "#         position_embedding = self.position_embedding(\n",
    "#             torch.LongTensor(range(seq_length))\n",
    "#         )\n",
    "#         token_type_embedding = self.token_type_embedding(\n",
    "#             torch.LongTensor(batch_size).fill_(0)\n",
    "#         )\n",
    "\n",
    "#         # Concatenate the embeddings and apply dropout\n",
    "#         embeddings = self.dropout(\n",
    "#             word_embedding + position_embedding + token_type_embedding\n",
    "#         )\n",
    "\n",
    "#         return embeddings\n",
    "# # You can then create an instance of the BERTEmbedding class by calling BERTEmbedding() and passing in the desired hyperparameters, such as the model size and dropout rate. For example:\n",
    "    \n",
    "    \n",
    "# # Define a BERT model subclass\n",
    "# class BERT(nn.Module):\n",
    "#     def __init__(self, h, d_model,d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Define and initialize the model's layers\n",
    "#         self.embedding = BERTEmbedding(d_model, dropout)\n",
    "#         self.attention = MultiHeadedAttention(h, d_model, dropout=dropout)\n",
    "#         self.feed_forward = PositionwiseFeedForward(d_model,d_ff, dropout=dropout)\n",
    "\n",
    "#         # Define the dropout and GELU activation function\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.activation = GELU()\n",
    "\n",
    "#         # Define sublayer connections\n",
    "#         self.attention_sublayer = SublayerConnection(d_model, dropout)\n",
    "#         self.feed_forward_sublayer = SublayerConnection(d_model, dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Perform the model's forward pass\n",
    "#         x = self.embedding(x)\n",
    "#         x = self.attention_sublayer(x, self.attention)\n",
    "#         x = self.feed_forward_sublayer(x, self.feed_forward)\n",
    "#         return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbceddf",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "Each type of embedding serves a different purpose in the BERT model.\n",
    "\n",
    "- The token embedding is used to represent each token (word or subword) in the input sequence. This is the main way that BERT represents the input text and is used as input to the rest of the model.\n",
    "- The segment embedding is used to represent the segment (sentence) that each token belongs to. This allows the model to differentiate between tokens in different sentences and apply appropriate processing.\n",
    "- The position embedding is used to represent the position of each token within the input sequence. This allows the model to take the relative position of the tokens into account when processing the input.\n",
    "\n",
    "Together, these embeddings provide BERT with a rich representation of the input text that it can use to perform various natural language understanding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2378b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=128):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "#     def __init__(self, d_model, max_len=99):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class SegmentEmbedding(nn.Embedding):\n",
    "#     def __init__(self, embed_size=128):\n",
    "#         super().__init__(3, embed_size, padding_idx=0)\n",
    "    def __init__(self, vocab_size, embed_size=128):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "#The self.token.embedding_dim attribute specifies the size of the embedding dimension for the tokens. In other words, it determines the length of the vectors that will be used to represent each token in the input sequence. This is a fixed property of the model and cannot be changed once it has been initialized.\n",
    "#          self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.token = TokenEmbedding(vocab_size=2*vocab_size+1, embed_size=embed_size)\n",
    "#         self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.position = PositionalEmbedding(max_len=99, d_model=self.token.embedding_dim)\n",
    "# max_len=100 is also ok!!!\n",
    "#         self.position = PositionalEmbedding(max_len=100, d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(vocab_size=vocab_size+1, embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b526c13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccebb09",
   "metadata": {},
   "source": [
    "AssertionError: \n",
    "The error message you are seeing indicates that there is an issue with the BERT model that you are trying to instantiate. The specific error is that the d_model argument must be divisible by the h argument, but this is not the case in your code.\n",
    "\n",
    "The d_model and h arguments correspond to the hidden size of the model and the number of attention heads, respectively. In order to fix the error, you need to make sure that the hidden size is divisible by the number of attention heads.\n",
    "```\n",
    "model = BERT(vocab_size=n_skill, hidden=128, attn_heads=4)\n",
    "model = BERT(vocab_size=n_skill, hidden=128*4, attn_heads=4)\n",
    "model = BERT(vocab_size=n_skill, hidden=128, attn_heads=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ef645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): BERTEmbedding(\n",
       "    (token): TokenEmbedding(27047, 128, padding_idx=0)\n",
       "    (position): PositionalEmbedding()\n",
       "    (segment): SegmentEmbedding(13524, 128, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (pred): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # chatGPT model \n",
    "# # Create a BERT model with 4 attention heads and a model size of 128\n",
    "\n",
    "# # Define the number of words in the input vocabulary\n",
    "# num_embeddings = 10000\n",
    "\n",
    "# # Define the maximum length of the input sequence\n",
    "# num_positions = 512\n",
    "\n",
    "# # Define the number of token types in the input\n",
    "# # For example, the BERT tokenizer used in the example code uses the \"bert-base-uncased\" pre-trained model, which uses a vocabulary of 30,522 tokens. This means that the BERT model will use 30,522 different token types to represent the input text. However, this is just one example, and the number of token types used by a BERT model can vary depending on the specific pre-trained model and tokenization scheme that is used.\n",
    "# num_token_types = 2\n",
    "\n",
    "\n",
    "# # Create a BERT embedding layer with a model size of 128, a vocabulary size of 10000, and a dropout rate of 0.1\n",
    "# # # Create a position-wise feed-forward network with a model size of 128 and a hidden layer size of 512\n",
    "# model = BERT(4, 128, 512)\n",
    "\n",
    "\n",
    "# Official BERT model\n",
    "model = BERT(vocab_size=n_skill,attn_heads=8)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b629148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb07572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d8690d",
   "metadata": {},
   "source": [
    "## Define Train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28bedd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dataloader, optimizer, scheduler, criterion, device=\"cpu\"):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    for item in dataloader:\n",
    "#         x = item[0]\n",
    "#         print('x',x, 'x.size()',x.size())\n",
    "        x = item[0].to(device).long()\n",
    "        segment_info = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, segment_info)\n",
    "#         print('output',output, 'output.size()',output.size())\n",
    "        \n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        target_mask = (segment_info != 0)  # Create a mask indicating which values in segment_info are not 0\n",
    "        last_nonzero_idx = target_mask.sum(dim=1) - 1  # Find the last non-zero value for each batch\n",
    "        \n",
    "        output = output[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the output tensor using last_nonzero_idx\n",
    "        label = label[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the label tensor using last_nonzero_idx\n",
    "\n",
    "#         print('output = output[range(64), last_nonzero_idx]', output,'output.size()',output.size())\n",
    "    \n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         output = torch.masked_select(output, target_mask)\n",
    "#         label = torch.masked_select(label, target_mask)\n",
    "#         pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "#         print('output = torch.masked_select(output, target_mask)', output,'output.size()',output.size())\n",
    "        \n",
    "#         print('label = torch.masked_select(label, target_mask)', label,'label.size()',label.size())\n",
    "#         print('pred = (torch.sigmoid(output) >= 0.5).long()', pred, 'pred.size()',pred.size())\n",
    "        \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(train_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72be72",
   "metadata": {},
   "source": [
    "## Define Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d44184f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(model, dataloader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    for item in dataloader:\n",
    "        x = item[0].to(device).long()\n",
    "        segment_info = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "    \n",
    "#         target_mask = (segment_info != 0)\n",
    "\n",
    "        output= model(x, segment_info)\n",
    "        loss = criterion(output, label)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "        target_mask = (segment_info != 0)  # Create a mask indicating which values in segment_info are not 0\n",
    "        \n",
    "        last_nonzero_idx = target_mask.sum(dim=1) - 1  # Find the last non-zero value for each batch\n",
    "        \n",
    "        output = output[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the output tensor using last_nonzero_idx\n",
    "        label = label[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the label tensor using last_nonzero_idx\n",
    "\n",
    "#         output = torch.masked_select(output, target_mask)\n",
    "#         label = torch.masked_select(label, target_mask)\n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "#         pred = (torch.sigmoid(output) >= 0.5)\n",
    "    \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(valid_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb870691",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dda76c",
   "metadata": {},
   "source": [
    "- **Criterion**: BCELoss and BCEWithLogitsLoss are both functions used to calculate the binary cross-entropy loss for a given set of predicted and target values. The main difference between the two is that BCEWithLogitsLoss applies a sigmoid function to the predicted values, whereas BCELoss expects the predicted values to already be in the range of 0 to 1. The negative log likelihood loss (which BERT use originally), also known as the cross-entropy loss, is a common loss function used in classification tasks, particularly when working with a multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824c11a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 1/10 train: loss-0.4613, acc-0.7570, auc - 0.8204\n",
      "epoch - 1/10 valid: loss-0.3776, [acc-0.7839, auc - 0.8666]\n",
      "epoch - 2/10 train: loss-0.2037, acc-0.7823, auc - 0.8632\n",
      "epoch - 2/10 valid: loss-0.0124, [acc-0.7887, auc - 0.8852]\n",
      "epoch - 3/10 train: loss-0.0186, acc-0.7889, auc - 0.8732\n",
      "epoch - 3/10 valid: loss-0.0111, [acc-0.7943, auc - 0.8863]\n",
      "epoch - 4/10 train: loss-0.0093, acc-0.7893, auc - 0.8747\n",
      "epoch - 4/10 valid: loss-0.0080, [acc-0.8014, auc - 0.8889]\n",
      "epoch - 5/10 train: loss-0.0076, acc-0.7935, auc - 0.8799\n",
      "epoch - 5/10 valid: loss-0.0094, [acc-0.8011, auc - 0.8883]\n",
      "epoch - 6/10 train: loss-0.0068, acc-0.7993, auc - 0.8859\n",
      "epoch - 6/10 valid: loss-0.0083, [acc-0.8047, auc - 0.8915]\n",
      "epoch - 7/10 train: loss-0.0063, acc-0.8049, auc - 0.8918\n",
      "epoch - 7/10 valid: loss-0.0072, [acc-0.8087, auc - 0.8948]\n",
      "epoch - 8/10 train: loss-0.0060, acc-0.8098, auc - 0.8973\n",
      "epoch - 8/10 valid: loss-0.0076, [acc-0.8075, auc - 0.8921]\n",
      "epoch - 9/10 train: loss-0.0057, acc-0.8144, auc - 0.9022\n",
      "epoch - 9/10 valid: loss-0.0075, [acc-0.8023, auc - 0.8941]\n",
      "epoch - 10/10 train: loss-0.0056, acc-0.8172, auc - 0.9054\n",
      "epoch - 10/10 valid: loss-0.0080, [acc-0.8056, auc - 0.8936]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=MAX_LEARNING_RATE, steps_per_epoch=len(train_dataloader), epochs=EPOCHS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "best_auc = 0\n",
    "max_steps = 3\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    loss, acc, auc = train_fn(model, train_dataloader, optimizer, scheduler, criterion, device)\n",
    "    print(\"epoch - {}/{} train: loss-{:.4f}, acc-{:.4f}, auc - {:.4f}\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "    loss, acc, auc = valid_fn(model, valid_dataloader, criterion, device)\n",
    "    print(\"epoch - {}/{} valid: loss-{:.4f}, [acc-{:.4f}, auc - {:.4f}]\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        step = 0\n",
    "        torch.save(model.state_dict(), \"bert_model.pt\")\n",
    "    else:\n",
    "        step += 1\n",
    "        if step >= max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc2bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ff967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfeff59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
